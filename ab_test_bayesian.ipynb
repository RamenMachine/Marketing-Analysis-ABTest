{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Marketing A/B Test - Bayesian Statistical Analysis\n",
        "\n",
        "**Author**: Analytics Team  \n",
        "**Date**: November 2025  \n",
        "**Version**: 1.0\n",
        "\n",
        "---\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "This notebook performs Bayesian statistical analysis using the Beta-Binomial model to estimate conversion rates and calculate probabilities of superiority. Bayesian methods provide intuitive probability statements that are directly interpretable for business decision-making.\n",
        "\n",
        "## Bayesian vs Frequentist Approach\n",
        "\n",
        "### Key Advantages of Bayesian Analysis\n",
        "\n",
        "- **Direct Probability Statements**: \"There is a 95% probability that the ad group is superior\"\n",
        "- **Prior Knowledge Integration**: Can incorporate historical data or expert knowledge\n",
        "- **Sequential Updates**: Beliefs update as new data arrives\n",
        "- **Posterior Distributions**: Full uncertainty quantification, not just point estimates\n",
        "- **Intuitive Interpretation**: Credible intervals have natural probability interpretation\n",
        "\n",
        "### Beta-Binomial Model\n",
        "\n",
        "We model conversion rates using a Beta-Binomial conjugate prior, which is mathematically elegant and computationally efficient for binary outcomes. The Beta distribution is the natural conjugate prior for the Binomial likelihood.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import beta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Preparation\n",
        "\n",
        "Load data and extract conversion counts for each group.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_csv('marketing_AB.csv')\n",
        "\n",
        "# Separate groups\n",
        "ad_group = df[df['test group'] == 'ad']\n",
        "psa_group = df[df['test group'] == 'psa']\n",
        "\n",
        "# Extract conversion data\n",
        "ad_conversions = ad_group['converted'].sum()\n",
        "ad_non_conversions = len(ad_group) - ad_conversions\n",
        "n_ad = len(ad_group)\n",
        "\n",
        "psa_conversions = psa_group['converted'].sum()\n",
        "psa_non_conversions = len(psa_group) - psa_conversions\n",
        "n_psa = len(psa_group)\n",
        "\n",
        "# Observed conversion rates\n",
        "cr_ad = ad_conversions / n_ad\n",
        "cr_psa = psa_conversions / n_psa\n",
        "\n",
        "print(f\"Ad Group:  {n_ad:,} users, {ad_conversions:,} conversions, Rate: {cr_ad:.6f}\")\n",
        "print(f\"PSA Group: {n_psa:,} users, {psa_conversions:,} conversions, Rate: {cr_psa:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Beta-Binomial Model\n",
        "\n",
        "### Prior Distribution\n",
        "\n",
        "We use a non-informative uniform prior: $\\text{Beta}(1, 1)$, which is equivalent to a uniform distribution on $[0,1]$.\n",
        "\n",
        "The Beta distribution has probability density function:\n",
        "\n",
        "$$f(p | \\alpha, \\beta) = \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha, \\beta)}$$\n",
        "\n",
        "where $B(\\alpha, \\beta)$ is the Beta function.\n",
        "\n",
        "### Posterior Distribution\n",
        "\n",
        "For a Beta prior and Binomial likelihood, the posterior is also Beta (conjugate prior):\n",
        "\n",
        "$$\\text{Posterior} = \\text{Beta}(\\alpha + \\text{successes}, \\beta + \\text{failures})$$\n",
        "\n",
        "### Posterior Mean\n",
        "\n",
        "The posterior mean (expected conversion rate) is:\n",
        "\n",
        "$$E[p | \\text{data}] = \\frac{\\alpha + \\text{successes}}{\\alpha + \\beta + \\text{total trials}}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prior parameters (non-informative uniform prior: Beta(1, 1))\n",
        "alpha_prior_ad = 1\n",
        "beta_prior_ad = 1\n",
        "alpha_prior_psa = 1\n",
        "beta_prior_psa = 1\n",
        "\n",
        "# Posterior parameters (Beta(alpha + successes, beta + failures))\n",
        "alpha_post_ad = alpha_prior_ad + ad_conversions\n",
        "beta_post_ad = beta_prior_ad + ad_non_conversions\n",
        "\n",
        "alpha_post_psa = alpha_prior_psa + psa_conversions\n",
        "beta_post_psa = beta_prior_psa + psa_non_conversions\n",
        "\n",
        "print(\"BETA-BINOMIAL MODEL\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Prior Distribution:\")\n",
        "print(f\"  Ad Group:  Beta({alpha_prior_ad}, {beta_prior_ad})\")\n",
        "print(f\"  PSA Group: Beta({alpha_prior_psa}, {beta_prior_psa})\")\n",
        "\n",
        "print(f\"\\nPosterior Distribution:\")\n",
        "print(f\"  Ad Group:  Beta({alpha_post_ad:.1f}, {beta_post_ad:.1f})\")\n",
        "print(f\"  PSA Group: Beta({alpha_post_psa:.1f}, {beta_post_psa:.1f})\")\n",
        "\n",
        "# Posterior means\n",
        "post_mean_ad = alpha_post_ad / (alpha_post_ad + beta_post_ad)\n",
        "post_mean_psa = alpha_post_psa / (alpha_post_psa + beta_post_psa)\n",
        "\n",
        "print(f\"\\nPosterior Mean Conversion Rates:\")\n",
        "print(f\"  Ad Group:  {post_mean_ad:.6f} ({post_mean_ad*100:.4f}%)\")\n",
        "print(f\"  PSA Group: {post_mean_psa:.6f} ({post_mean_psa*100:.4f}%)\")\n",
        "print(f\"  Expected Lift: {(post_mean_ad - post_mean_psa) / post_mean_psa * 100:.4f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Credible Intervals\n",
        "\n",
        "Credible intervals are the Bayesian equivalent of confidence intervals, but with a more intuitive interpretation.\n",
        "\n",
        "### Credible Interval\n",
        "\n",
        "A $(1-\\alpha)$ credible interval contains $(1-\\alpha)$ of the posterior probability:\n",
        "\n",
        "$$P(p \\in [L, U] | \\text{data}) = 1 - \\alpha$$\n",
        "\n",
        "We can find $L$ and $U$ such that:\n",
        "- $P(p < L | \\text{data}) = \\alpha/2$\n",
        "- $P(p > U | \\text{data}) = \\alpha/2$\n",
        "\n",
        "For a Beta distribution, we use the inverse CDF (percentile function).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: Posterior Distributions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Posterior distributions\n",
        "x = np.linspace(0.10, 0.12, 1000)\n",
        "post_ad = beta.pdf(x, alpha_post_ad, beta_post_ad)\n",
        "post_psa = beta.pdf(x, alpha_post_psa, beta_post_psa)\n",
        "\n",
        "axes[0].plot(x, post_ad, label='Ad Group Posterior', linewidth=2.5, color='#3B82F6')\n",
        "axes[0].plot(x, post_psa, label='PSA Group Posterior', linewidth=2.5, color='#6B7280')\n",
        "axes[0].axvline(post_mean_ad, color='#3B82F6', linestyle='--', linewidth=2, alpha=0.7, label=f'Ad Mean: {post_mean_ad:.4f}')\n",
        "axes[0].axvline(post_mean_psa, color='#6B7280', linestyle='--', linewidth=2, alpha=0.7, label=f'PSA Mean: {post_mean_psa:.4f}')\n",
        "axes[0].fill_between(x, post_ad, alpha=0.3, color='#3B82F6')\n",
        "axes[0].fill_between(x, post_psa, alpha=0.3, color='#6B7280')\n",
        "axes[0].set_xlabel('Conversion Rate', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Probability Density', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Posterior Distributions of Conversion Rates', fontsize=14, fontweight='bold', pad=20)\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(alpha=0.3, linestyle='--')\n",
        "\n",
        "# Difference distribution\n",
        "diff_x = np.linspace(posterior_diff_samples.min(), posterior_diff_samples.max(), 100)\n",
        "axes[1].hist(posterior_diff_samples, bins=50, density=True, alpha=0.7, color='green', edgecolor='black', linewidth=1.5)\n",
        "axes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero (No Difference)')\n",
        "axes[1].axvline(diff_ci_lower, color='orange', linestyle='--', linewidth=2, label=f'95% CI Lower: {diff_ci_lower:.6f}')\n",
        "axes[1].axvline(diff_ci_upper, color='orange', linestyle='--', linewidth=2, label=f'95% CI Upper: {diff_ci_upper:.6f}')\n",
        "axes[1].set_xlabel('Difference (Ad - PSA)', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Probability Density', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('Posterior Distribution of Difference', fontsize=14, fontweight='bold', pad=20)\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(alpha=0.3, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate samples from posterior distributions\n",
        "n_samples = 100000\n",
        "posterior_ad_samples = np.random.beta(alpha_post_ad, beta_post_ad, n_samples)\n",
        "posterior_psa_samples = np.random.beta(alpha_post_psa, beta_post_psa, n_samples)\n",
        "\n",
        "# Difference distribution\n",
        "posterior_diff_samples = posterior_ad_samples - posterior_psa_samples\n",
        "\n",
        "# 95% Credible intervals\n",
        "ci_level = 0.95\n",
        "alpha_ci = 1 - ci_level\n",
        "\n",
        "# For Ad group\n",
        "ad_ci_lower = np.percentile(posterior_ad_samples, 100 * alpha_ci / 2)\n",
        "ad_ci_upper = np.percentile(posterior_ad_samples, 100 * (1 - alpha_ci / 2))\n",
        "\n",
        "# For PSA group\n",
        "psa_ci_lower = np.percentile(posterior_psa_samples, 100 * alpha_ci / 2)\n",
        "psa_ci_upper = np.percentile(posterior_psa_samples, 100 * (1 - alpha_ci / 2))\n",
        "\n",
        "# For difference\n",
        "diff_ci_lower = np.percentile(posterior_diff_samples, 100 * alpha_ci / 2)\n",
        "diff_ci_upper = np.percentile(posterior_diff_samples, 100 * (1 - alpha_ci / 2))\n",
        "\n",
        "print(\"CREDIBLE INTERVALS (95%)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Ad Group Conversion Rate:\")\n",
        "print(f\"  [{ad_ci_lower:.6f}, {ad_ci_upper:.6f}]\")\n",
        "print(f\"  [{(ad_ci_lower*100):.4f}%, {(ad_ci_upper*100):.4f}%]\")\n",
        "\n",
        "print(f\"\\nPSA Group Conversion Rate:\")\n",
        "print(f\"  [{psa_ci_lower:.6f}, {psa_ci_upper:.6f}]\")\n",
        "print(f\"  [{(psa_ci_lower*100):.4f}%, {(psa_ci_upper*100):.4f}%]\")\n",
        "\n",
        "print(f\"\\nDifference (Ad - PSA):\")\n",
        "print(f\"  [{diff_ci_lower:.6f}, {diff_ci_upper:.6f}]\")\n",
        "print(f\"  [{(diff_ci_lower*100):.4f}%, {(diff_ci_upper*100):.4f}%]\")\n",
        "\n",
        "if diff_ci_lower > 0:\n",
        "    print(f\"\\n✅ Credible interval excludes zero - Ad group is superior\")\n",
        "elif diff_ci_upper < 0:\n",
        "    print(f\"\\n✅ Credible interval excludes zero - PSA group is superior\")\n",
        "else:\n",
        "    print(f\"\\n⚠️  Credible interval includes zero - No clear superiority\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Probability of Superiority\n",
        "\n",
        "One of the key advantages of Bayesian analysis is the ability to make direct probability statements.\n",
        "\n",
        "### Probability of Superiority\n",
        "\n",
        "We calculate:\n",
        "\n",
        "$$P(p_{\\text{ad}} > p_{\\text{psa}} | \\text{data})$$\n",
        "\n",
        "This is the probability that the ad group has a higher conversion rate than the psa group, given the observed data.\n",
        "\n",
        "We estimate this by sampling from the posterior distributions and calculating the proportion of samples where $p_{\\text{ad}} > p_{\\text{psa}}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Probability that Ad > PSA\n",
        "prob_ad_better = np.mean(posterior_ad_samples > posterior_psa_samples)\n",
        "prob_psa_better = np.mean(posterior_psa_samples > posterior_ad_samples)\n",
        "prob_equal = 1 - prob_ad_better - prob_psa_better\n",
        "\n",
        "print(\"PROBABILITY OF SUPERIORITY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"P(Ad > PSA): {prob_ad_better:.6f} ({prob_ad_better*100:.4f}%)\")\n",
        "print(f\"P(PSA > Ad): {prob_psa_better:.6f} ({prob_psa_better*100:.4f}%)\")\n",
        "print(f\"P(Equal):    {prob_equal:.6f} ({prob_equal*100:.4f}%)\")\n",
        "\n",
        "# Probability of meaningful lift (e.g., > 1% relative lift)\n",
        "min_lift = 0.01\n",
        "prob_meaningful_lift = np.mean((posterior_ad_samples - posterior_psa_samples) / posterior_psa_samples > min_lift)\n",
        "print(f\"\\nProbability of >{min_lift*100:.0f}% relative lift:\")\n",
        "print(f\"  P(Lift > {min_lift*100:.0f}%): {prob_meaningful_lift:.6f} ({prob_meaningful_lift*100:.4f}%)\")\n",
        "\n",
        "# Interpretation\n",
        "if prob_ad_better > 0.95:\n",
        "    interpretation = \"Very Strong Evidence\"\n",
        "elif prob_ad_better > 0.90:\n",
        "    interpretation = \"Strong Evidence\"\n",
        "elif prob_ad_better > 0.75:\n",
        "    interpretation = \"Moderate Evidence\"\n",
        "elif prob_ad_better > 0.50:\n",
        "    interpretation = \"Weak Evidence\"\n",
        "else:\n",
        "    interpretation = \"No Evidence\"\n",
        "\n",
        "print(f\"\\n{interpretation} that Ad group is superior\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Expected Value Calculations\n",
        "\n",
        "We can use the posterior distribution to estimate expected business impact.\n",
        "\n",
        "### Expected Incremental Conversions\n",
        "\n",
        "$$\\text{Expected Incremental Conversions} = (E[p_{\\text{ad}}] - E[p_{\\text{psa}}]) \\times n_{\\text{ad}}$$\n",
        "\n",
        "### Expected Incremental Revenue\n",
        "\n",
        "$$\\text{Expected Revenue} = \\text{Expected Incremental Conversions} \\times \\text{Value per Conversion}$$\n",
        "\n",
        "We can also calculate the full distribution of expected revenue by sampling from the posterior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Conclusions\n",
        "\n",
        "### Bayesian Analysis Results\n",
        "- **Probability Ad > PSA**: {prob_ad_better:.2%}\n",
        "- **Expected Lift**: {(post_mean_ad - post_mean_psa) / post_mean_psa * 100:.4f}%\n",
        "- **Expected Incremental Revenue**: ${expected_incremental_revenue:,.2f}\n",
        "- **95% Credible Interval**: [{diff_ci_lower:.6f}, {diff_ci_upper:.6f}]\n",
        "\n",
        "### Key Findings\n",
        "- {interpretation} that Ad group is superior\n",
        "- The posterior distribution provides full uncertainty quantification\n",
        "- Business impact can be directly estimated from the posterior\n",
        "\n",
        "### Recommendations\n",
        "Based on the Bayesian analysis, we can make direct probability statements about the campaign's effectiveness and expected business impact.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary and save results\n",
        "summary = {\n",
        "    'ad_group_size': n_ad,\n",
        "    'psa_group_size': n_psa,\n",
        "    'ad_conversions': ad_conversions,\n",
        "    'psa_conversions': psa_conversions,\n",
        "    'ad_conversion_rate': cr_ad,\n",
        "    'psa_conversion_rate': cr_psa,\n",
        "    'posterior_mean_ad': post_mean_ad,\n",
        "    'posterior_mean_psa': post_mean_psa,\n",
        "    'expected_lift': (post_mean_ad - post_mean_psa) / post_mean_psa,\n",
        "    'prob_ad_better': prob_ad_better,\n",
        "    'prob_meaningful_lift': prob_meaningful_lift,\n",
        "    'credible_interval_lower': diff_ci_lower,\n",
        "    'credible_interval_upper': diff_ci_upper,\n",
        "    'expected_incremental_conversions': expected_incremental_conversions,\n",
        "    'expected_incremental_revenue': expected_incremental_revenue,\n",
        "    'revenue_ci_lower': revenue_ci_lower,\n",
        "    'revenue_ci_upper': revenue_ci_upper\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('bayesian_results.json', 'w') as f:\n",
        "    json.dump({k: float(v) if isinstance(v, (np.integer, np.floating)) else v \n",
        "              for k, v in summary.items()}, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BAYESIAN ANALYSIS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nProbability Ad > PSA: {prob_ad_better:.2%}\")\n",
        "print(f\"Expected Lift: {(post_mean_ad - post_mean_psa) / post_mean_psa * 100:.4f}%\")\n",
        "print(f\"Expected Incremental Revenue: ${expected_incremental_revenue:,.2f}\")\n",
        "print(f\"95% Credible Interval: [{diff_ci_lower:.6f}, {diff_ci_upper:.6f}]\")\n",
        "print(f\"\\n{interpretation} that Ad group is superior\")\n",
        "print(\"\\n✅ Results saved to 'bayesian_results.json'\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assume a value per conversion (e.g., $100)\n",
        "value_per_conversion = 100\n",
        "\n",
        "# Expected incremental conversions\n",
        "expected_incremental_conversions = (post_mean_ad - post_mean_psa) * n_ad\n",
        "\n",
        "# Expected incremental revenue\n",
        "expected_incremental_revenue = expected_incremental_conversions * value_per_conversion\n",
        "\n",
        "print(\"EXPECTED BUSINESS IMPACT\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Value per conversion: ${value_per_conversion}\")\n",
        "print(f\"Expected incremental conversion rate: {post_mean_ad - post_mean_psa:.6f}\")\n",
        "print(f\"Expected incremental conversions: {expected_incremental_conversions:.2f}\")\n",
        "print(f\"Expected incremental revenue: ${expected_incremental_revenue:,.2f}\")\n",
        "\n",
        "# Distribution of expected revenue\n",
        "revenue_samples = (posterior_ad_samples - posterior_psa_samples) * n_ad * value_per_conversion\n",
        "revenue_ci_lower = np.percentile(revenue_samples, 2.5)\n",
        "revenue_ci_upper = np.percentile(revenue_samples, 97.5)\n",
        "\n",
        "print(f\"\\nRevenue Distribution:\")\n",
        "print(f\"  Mean: ${revenue_samples.mean():,.2f}\")\n",
        "print(f\"  95% Credible Interval: [${revenue_ci_lower:,.2f}, ${revenue_ci_upper:,.2f}]\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
